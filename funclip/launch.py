#!/usr/bin/env python3
# -*- encoding: utf-8 -*-
# Copyright FunASR (https://github.com/alibaba-damo-academy/FunClip). All Rights Reserved.
#  MIT License  (https://opensource.org/licenses/MIT)

from http import server
import os
import logging
import argparse
import gradio as gr
from funasr import AutoModel
from videoclipper import VideoClipper
from llm.openai_api import openai_call
from llm.qwen_api import call_qwen_model
from llm.g4f_openai_api import g4f_openai_call
from llm.private_api import openai_call
from utils.trans_utils import extract_timestamps
from dotenv import load_dotenv

# åŠ è½½ .env æ–‡ä»¶
load_dotenv()

# è·å–è´¦å·å’Œå¯†ç ï¼Œè®¾ç½®é»˜è®¤å€¼
DEFAULT_USERNAME = os.getenv("USERNAME", "motor")
DEFAULT_PASSWORD = os.getenv("PASSWORD", "admin")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='argparse testing')
    parser.add_argument('--lang', '-l', type=str, default = "zh", help="language")
    parser.add_argument('--share', '-s', action='store_true', help="if to establish gradio share link")
    parser.add_argument('--port', '-p', type=int, default=7860, help='port number')
    parser.add_argument('--listen', action='store_true', help="if to listen to all hosts")
    args = parser.parse_args()
    
    if args.lang == 'zh':
        funasr_model = AutoModel(model="iic/speech_seaco_paraformer_large_asr_nat-zh-cn-16k-common-vocab8404-pytorch",
                                vad_model="damo/speech_fsmn_vad_zh-cn-16k-common-pytorch",
                                punc_model="damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch",
                                spk_model="damo/speech_campplus_sv_zh-cn_16k-common",
                                )
    else:
        funasr_model = AutoModel(model="iic/speech_paraformer_asr-en-16k-vocab4199-pytorch",
                                vad_model="damo/speech_fsmn_vad_zh-cn-16k-common-pytorch",
                                punc_model="damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch",
                                spk_model="damo/speech_campplus_sv_zh-cn_16k-common",
                                )
    audio_clipper = VideoClipper(funasr_model)
    audio_clipper.lang = args.lang
    
    server_name='127.0.0.1'
    if args.listen:
        server_name = '0.0.0.0'
        

    def audio_recog(audio_input, sd_switch, hotwords, output_dir):
        return audio_clipper.recog(audio_input, sd_switch, None, hotwords, output_dir=output_dir)

    def video_recog(video_input, sd_switch, hotwords, output_dir):
        return audio_clipper.video_recog(video_input, sd_switch, hotwords, output_dir=output_dir)

    def video_clip(dest_text, video_spk_input, start_ost, end_ost, state, output_dir):
        return audio_clipper.video_clip(
            dest_text, start_ost, end_ost, state, dest_spk=video_spk_input, output_dir=output_dir
            )

    def mix_recog(video_input, audio_input, hotwords, output_dir):
        output_dir = output_dir.strip()
        if not len(output_dir):
            output_dir = None
        else:
            output_dir = os.path.abspath(output_dir)
        audio_state, video_state = None, None
        if video_input is not None:
            res_text, res_srt, video_state = video_recog(
                video_input, 'No', hotwords, output_dir=output_dir)
            return res_text, res_srt, video_state, None
        if audio_input is not None:
            res_text, res_srt, audio_state = audio_recog(
                audio_input, 'No', hotwords, output_dir=output_dir)
            return res_text, res_srt, None, audio_state
    
    def mix_recog_speaker(video_input, audio_input, hotwords, output_dir):
        output_dir = output_dir.strip()
        if not len(output_dir):
            output_dir = None
        else:
            output_dir = os.path.abspath(output_dir)
        audio_state, video_state = None, None
        if video_input is not None:
            res_text, res_srt, video_state = video_recog(
                video_input, 'Yes', hotwords, output_dir=output_dir)
            return res_text, res_srt, video_state, None
        if audio_input is not None:
            res_text, res_srt, audio_state = audio_recog(
                audio_input, 'Yes', hotwords, output_dir=output_dir)
            return res_text, res_srt, None, audio_state
    
    def mix_clip(dest_text, video_spk_input, start_ost, end_ost, video_state, audio_state, output_dir):
        output_dir = output_dir.strip()
        if not len(output_dir):
            output_dir = None
        else:
            output_dir = os.path.abspath(output_dir)
        if video_state is not None:
            clip_video_file, message, clip_srt = audio_clipper.video_clip(
                dest_text, start_ost, end_ost, video_state, dest_spk=video_spk_input, output_dir=output_dir)
            return clip_video_file, None, message, clip_srt
        if audio_state is not None:
            (sr, res_audio), message, clip_srt = audio_clipper.clip(
                dest_text, start_ost, end_ost, audio_state, dest_spk=video_spk_input, output_dir=output_dir)
            return None, (sr, res_audio), message, clip_srt
    
    def video_clip_addsub(dest_text, video_spk_input, start_ost, end_ost, state, output_dir, font_size, font_color):
        output_dir = output_dir.strip()
        if not len(output_dir):
            output_dir = None
        else:
            output_dir = os.path.abspath(output_dir)
        return audio_clipper.video_clip(
            dest_text, start_ost, end_ost, state, 
            font_size=font_size, font_color=font_color, 
            add_sub=True, dest_spk=video_spk_input, output_dir=output_dir
            )
        
    # def llm_inference(system_content, user_content, srt_text, model, apikey):
    #     SUPPORT_LLM_PREFIX = ['qwen', 'gpt', 'g4f', 'moonshot']
    #     if model.startswith('qwen'):
    #         return call_qwen_model(apikey, model, user_content+'\n'+srt_text, system_content)
    #     if model.startswith('gpt') or model.startswith('moonshot'):
    #         return openai_call(apikey, model, system_content, user_content+'\n'+srt_text)
    #     elif model.startswith('g4f'):
    #         model = "-".join(model.split('-')[1:])
    #         return g4f_openai_call(model, system_content, user_content+'\n'+srt_text)
    #     else:
    #         logging.error("LLM name error, only {} are supported as LLM name prefix."
    #                       .format(SUPPORT_LLM_PREFIX))

    def llm_inference(system_content, user_content, srt_text, model, apikey, api_base=None):
        """
        This function will check for the model prefix and call the appropriate API method (in this case, OpenAI).
        """
        return openai_call(apikey, model, system_content, user_content+'\n'+srt_text, api_base=api_base)
    
    def AI_clip(LLM_res, dest_text, video_spk_input, start_ost, end_ost, video_state, audio_state, output_dir):
        timestamp_list = extract_timestamps(LLM_res)
        output_dir = output_dir.strip()
        if not len(output_dir):
            output_dir = None
        else:
            output_dir = os.path.abspath(output_dir)
        if video_state is not None:
            clip_video_file, message, clip_srt = audio_clipper.video_clip(
                dest_text, start_ost, end_ost, video_state, 
                dest_spk=video_spk_input, output_dir=output_dir, timestamp_list=timestamp_list, add_sub=False)
            return clip_video_file, None, message, clip_srt
        if audio_state is not None:
            (sr, res_audio), message, clip_srt = audio_clipper.clip(
                dest_text, start_ost, end_ost, audio_state, 
                dest_spk=video_spk_input, output_dir=output_dir, timestamp_list=timestamp_list, add_sub=False)
            return None, (sr, res_audio), message, clip_srt
    
    def AI_clip_subti(LLM_res, dest_text, video_spk_input, start_ost, end_ost, video_state, audio_state, output_dir):
        timestamp_list = extract_timestamps(LLM_res)
        output_dir = output_dir.strip()
        if not len(output_dir):
            output_dir = None
        else:
            output_dir = os.path.abspath(output_dir)
        if video_state is not None:
            clip_video_file, message, clip_srt = audio_clipper.video_clip(
                dest_text, start_ost, end_ost, video_state, 
                dest_spk=video_spk_input, output_dir=output_dir, timestamp_list=timestamp_list, add_sub=True)
            return clip_video_file, None, message, clip_srt
        if audio_state is not None:
            (sr, res_audio), message, clip_srt = audio_clipper.clip(
                dest_text, start_ost, end_ost, audio_state, 
                dest_spk=video_spk_input, output_dir=output_dir, timestamp_list=timestamp_list, add_sub=True)
            return None, (sr, res_audio), message, clip_srt
        
    def extract_speaker_labels(subtitles):
        """
        ä»å­—å¹•å†…å®¹ä¸­æå–æ‰€æœ‰çš„ spkX æ ‡è¯†ï¼Œå¹¶ç”Ÿæˆé»˜è®¤çš„æ˜ å°„å­—ç¬¦ä¸²ã€‚
        :param subtitles: SRTå­—å¹•å†…å®¹
        :return: é»˜è®¤çš„è¯´è¯äººæ˜ å°„å­—ç¬¦ä¸² (å¦‚ "spk0:, spk1:, spk2:")
        """
        import re
        speakers = sorted(set(re.findall(r"\bspk\d+\b", subtitles)))
        return "\n".join(f"{speaker}:" for speaker in speakers)

    def prepare_speaker_map(subtitles):
        """
        æ ¹æ®å­—å¹•å†…å®¹åŠ¨æ€ç”Ÿæˆé¢„å¡«å……çš„è¯´è¯äººæ˜ å°„è§„åˆ™ã€‚
        :param subtitles: å½“å‰ SRT å­—å¹•å†…å®¹
        :return: é»˜è®¤çš„è¯´è¯äººæ˜ å°„è§„åˆ™å­—ç¬¦ä¸²
        """
        return extract_speaker_labels(subtitles)

    def parse_speaker_map(speaker_map_str):
        """
        è§£æç”¨æˆ·è¾“å…¥çš„è¯´è¯äººæ˜ å°„å­—ç¬¦ä¸²ä¸ºå­—å…¸ã€‚
        :param speaker_map_str: è¯´è¯äººæ˜ å°„å­—ç¬¦ä¸²ï¼Œæ¯è¡Œä¸€ä¸ªæ˜ å°„ (å¦‚ "spk0:å¼ ä¸‰\nspk1:\nspk2:ç‹äº”")
        :return: è§£æåçš„å­—å…¸ (å¦‚ {"spk0": "å¼ ä¸‰", "spk2": "ç‹äº”"})
        """
        try:
            return {
                item.split(":", 1)[0].strip(): item.split(":", 1)[1].strip()
                for item in speaker_map_str.splitlines()
                if ":" in item.strip() and item.split(":", 1)[1].strip()  # å¿½ç•¥ç©ºå€¼è¡Œ
            }
        except ValueError:
            raise ValueError("è¯´è¯äººæ˜ å°„æ ¼å¼é”™è¯¯ï¼Œè¯·è¾“å…¥æ­£ç¡®çš„æ ¼å¼ï¼Œä¾‹å¦‚æ¯è¡Œ 'spk0:å¼ ä¸‰'")


    def replace_speaker_in_subtitles(subtitles, speaker_map_str):
        """
        æ›¿æ¢å­—å¹•ä¸­çš„è¯´è¯äººæ ‡è¯†ã€‚
        :param subtitles: åŸå§‹SRTå­—å¹•å†…å®¹ï¼ˆå­—ç¬¦ä¸²ï¼‰
        :param speaker_map_str: ç”¨æˆ·æä¾›çš„è¯´è¯äººæ˜ å°„å­—ç¬¦ä¸² (å¦‚ "spk0:å¼ ä¸‰, spk1:æå››")
        :return: æ›¿æ¢åçš„SRTå­—å¹•å†…å®¹
        """
        # å°†æ˜ å°„å­—ç¬¦ä¸²è§£æä¸ºå­—å…¸
        speaker_map = parse_speaker_map(speaker_map_str)
        
        # æ›¿æ¢SRTå†…å®¹ä¸­çš„è¯´è¯äºº
        def replace_speaker_line(line):
            for old_speaker, new_speaker in speaker_map.items():
                line = line.replace(old_speaker, new_speaker)
            return line

        # é€è¡Œæ›¿æ¢SRTå†…å®¹
        lines = subtitles.split("\n")
        replaced_lines = [replace_speaker_line(line) for line in lines]
        return "\n".join(replaced_lines)

    def summarize_asr(system_prompt, user_prompt, asr_text, model, apikey, api_base=None):
        """
        è°ƒç”¨LLMæ€»ç»“ASRè¯†åˆ«å†…å®¹
        :param system_prompt: ç³»ç»Ÿè®¾å®šçš„Prompt
        :param user_prompt: ç”¨æˆ·è¾“å…¥çš„Prompt
        :param asr_text: ASRè¯†åˆ«çš„ç»“æœ
        :param model: LLMæ¨¡å‹åç§°
        :param apikey: ç”¨æˆ·æä¾›çš„APIå¯†é’¥
        :param api_base: è‡ªå»ºAPIçš„Base URLï¼Œé»˜è®¤ä¸ºNone
        :return: LLMè¿”å›çš„æ€»ç»“ç»“æœ
        """
        return llm_inference(system_prompt, user_prompt, asr_text, model, apikey, api_base=api_base)


    # gradio interface
    theme = gr.Theme.load("funclip/utils/theme.json")
    with gr.Blocks(theme=theme) as funclip_service:

        video_state, audio_state = gr.State(), gr.State()
        with gr.Row():
            with gr.Column():
                with gr.Row():
                    video_input = gr.Video(label="è§†é¢‘è¾“å…¥ | Video Input")
                    audio_input = gr.Audio(label="éŸ³é¢‘è¾“å…¥ | Audio Input")
                with gr.Column():
                    gr.Examples(['https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ClipVideo/%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%A4%9A%E8%AF%BB%E4%B9%A6%EF%BC%9F%E8%BF%99%E6%98%AF%E6%88%91%E5%90%AC%E8%BF%87%E6%9C%80%E5%A5%BD%E7%9A%84%E7%AD%94%E6%A1%88-%E7%89%87%E6%AE%B5.mp4', 
                                 'https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ClipVideo/2022%E4%BA%91%E6%A0%96%E5%A4%A7%E4%BC%9A_%E7%89%87%E6%AE%B52.mp4', 
                                 'https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ClipVideo/%E4%BD%BF%E7%94%A8chatgpt_%E7%89%87%E6%AE%B5.mp4'],
                                [video_input],
                                label='ç¤ºä¾‹è§†é¢‘ | Demo Video')
                    gr.Examples(['https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ClipVideo/%E8%AE%BF%E8%B0%88.mp4'],
                                [video_input],
                                label='å¤šè¯´è¯äººç¤ºä¾‹è§†é¢‘ | Multi-speaker Demo Video')
                    gr.Examples(['https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ClipVideo/%E9%B2%81%E8%82%83%E9%87%87%E8%AE%BF%E7%89%87%E6%AE%B51.wav'],
                                [audio_input],
                                label="ç¤ºä¾‹éŸ³é¢‘ | Demo Audio")
                    with gr.Column():
                        # with gr.Row():
                            # video_sd_switch = gr.Radio(["No", "Yes"], label="ğŸ‘¥åŒºåˆ†è¯´è¯äºº Get Speakers", value='No')
                        hotwords_input = gr.Textbox(label="ğŸš’ çƒ­è¯ | Hotwords(å¯ä»¥ä¸ºç©ºï¼Œå¤šä¸ªçƒ­è¯ä½¿ç”¨ç©ºæ ¼åˆ†éš”ï¼Œä»…æ”¯æŒä¸­æ–‡çƒ­è¯)")
                        output_dir = gr.Textbox(label="ğŸ“ æ–‡ä»¶è¾“å‡ºè·¯å¾„ | File Output Dir (å¯ä»¥ä¸ºç©ºï¼ŒLinux, macç³»ç»Ÿå¯ä»¥ç¨³å®šä½¿ç”¨)", value=" ")
                        with gr.Row():
                            recog_button = gr.Button("ğŸ‘‚ è¯†åˆ« | ASR", variant="primary")
                            recog_button2 = gr.Button("ğŸ‘‚ğŸ‘« è¯†åˆ«+åŒºåˆ†è¯´è¯äºº | ASR+SD")
                video_text_output = gr.Textbox(label="âœï¸ è¯†åˆ«ç»“æœ | Recognition Result")
                video_srt_output = gr.Textbox(label="ğŸ“– SRTå­—å¹•å†…å®¹ | SRT Subtitles")
            with gr.Column():
                # æ›¿æ¢è¯´è¯äºº Tab
                with gr.Tab("ğŸ”„ æ›¿æ¢è¯´è¯äºº Replace Speaker"):
                    speaker_map_input = gr.Textbox(
                        label="æ›¿æ¢è§„åˆ™ | Replacement Rules (æ¯è¡Œæ ¼å¼: spkX:åç§°)",
                        placeholder="è‡ªåŠ¨ç”Ÿæˆå½“å‰å­—å¹•çš„spkXæ ‡è¯†...",
                        lines=10,  # å…è®¸å¤šè¡Œè¾“å…¥
                    )
                    replace_button = gr.Button("æ›¿æ¢ Replace", variant="primary")
                    replaced_srt_output = gr.Textbox(label="æ›¿æ¢åçš„SRTå­—å¹•å†…å®¹ | Replaced SRT Subtitles")

                    # è‡ªåŠ¨å¡«å…… speaker_map_input çš„é»˜è®¤å€¼
                    video_srt_output.change(
                        prepare_speaker_map,
                        inputs=[video_srt_output],
                        outputs=[speaker_map_input],
                    )

                    replace_button.click(
                        replace_speaker_in_subtitles,
                        inputs=[video_srt_output, speaker_map_input],
                        outputs=[replaced_srt_output],
                    )

                with gr.Tab("ğŸ“„ LLMæ–‡æ¡£æ€»ç»“ | LLM Document Summarization"):
                    with gr.Column():
                        # ç³»ç»ŸPromptè¾“å…¥æ¡†
                        system_prompt_input = gr.Textbox(
                            label="Prompt System (ç³»ç»Ÿæç¤ºè¯)",
                            placeholder="è¯·è¾“å…¥ç³»ç»ŸPromptï¼Œä¾‹å¦‚ï¼šä½ æ˜¯ä¸€ä¸ªè¯­éŸ³è¯†åˆ«æ€»ç»“åŠ©æ‰‹...",
                            value=("ä½ æ˜¯ä¸€ä¸ªè¯­éŸ³è¯†åˆ«æ€»ç»“åŠ©æ‰‹ï¼Œæ¥æ”¶ç”¨æˆ·æä¾›çš„è¯­éŸ³è½¬æ–‡æœ¬ï¼ˆASRï¼‰ç»“æœï¼Œæ ¹æ®ç”¨æˆ·æŒ‡ä»¤æ€»ç»“å…³é”®ä¿¡æ¯æˆ–ç”Ÿæˆå†…å®¹ã€‚"
                                "è¯­éŸ³è½¬æ–‡æœ¬çš„ç»“æœä»¥srtçš„å½¢å¼æä¾›ï¼Œä¾‹å¦‚ï¼š\n"
                                "0  spk0\n"
                                "00:00:00,50 --> 00:00:09,810\n"
                                "è¯­å¥1\n"
                                "1  spk1\n"
                                "00:00:10,270 --> 00:00:12,150\n"
                                "è¯­å¥2\n"
                                "2  spk1\n"
                                "00:00:12,790 --> 00:00:13,890\n"
                                "è¯­å¥3\n"
                                "å¦‚æœåºå·åæ²¡æœ‰è¯´è¯äººåï¼Œåˆ™ä¸ºä¸åŒºåˆ†è¯´è¯äººasrç»“æœ")
                        )

                        user_prompt_input = gr.Textbox(
                            label="Prompt User (ç”¨æˆ·è‡ªå®šä¹‰æç¤ºè¯)",
                            placeholder="è¯·è¾“å…¥ç”¨æˆ·Promptï¼Œä¾‹å¦‚ï¼šæ€»ç»“ä»¥ä¸‹å†…å®¹çš„å…³é”®ä¿¡æ¯...",
                            value="æ€»ç»“ä»¥ä¸‹å†…å®¹çš„å…³é”®ä¿¡æ¯ï¼Œè®¨è®ºçš„ä¸»é¢˜ã€çºªè¦ã€è¦ç‚¹å’Œæ€»ç»“ï¼Œä¸åŒå‘è¨€è€…çš„è§‚ç‚¹ï¼ˆå¦‚æœ‰ï¼‰ï¼Œä»¥åŠç»“è®ºå’Œç›®æ ‡ï¼ˆå¦‚æœ‰ï¼‰"
                        )

                        # LLMæ¨¡å‹é€‰æ‹©æ¡†
                        llm_model = gr.Dropdown(
                            choices=["qwen2.5:32b", "gpt-3.5-turbo", "gpt-4o"],
                            value="qwen2.5:32b",
                            label="LLM Model Name",
                            allow_custom_value=True
                        )

                        # å¯†é’¥è¾“å…¥æ¡†ï¼Œéšè—æ˜¾ç¤ºå¯†ç 
                        apikey_input = gr.Textbox(
                            label="APIKEY",
                            placeholder="è¾“å…¥API Keyï¼ˆå¦‚éœ€ä½¿ç”¨GPTæˆ–Qwen APIï¼‰",
                            type="password",  # é»˜è®¤éšè—
                        )

                        # è‡ªå»ºAPI Base URL è¾“å…¥æ¡†
                        api_base_input = gr.Textbox(
                            label="API Base (å¯é€‰)",
                            placeholder="è¯·è¾“å…¥è‡ªå»ºAPI Baseï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰",
                            lines=1
                        )

                        # æ€»ç»“æŒ‰é’®
                        summarize_button = gr.Button(
                            "æ€»ç»“ Summarize (ä½¿ç”¨LLMæ€»ç»“ASRå†…å®¹)",
                            variant="primary"
                        )

                        # æ€»ç»“ç»“æœæ˜¾ç¤ºæ¡†
                        llm_summary_result = gr.Textbox(
                            label="æ€»ç»“ç»“æœ | Summary Result",
                            placeholder="LLMè¿”å›çš„æ€»ç»“ç»“æœå°†æ˜¾ç¤ºåœ¨è¿™é‡Œ"
                        )

                        # æ­£ç¡®ä¼ é€’è¾“å…¥å†…å®¹ç»™å‡½æ•°
                        def get_valid_srt_output(video_srt, replaced_srt):
                            """
                            Check which SRT content is valid: replaced or original.
                            :param video_srt: Original SRT content
                            :param replaced_srt: Replaced SRT content
                            :return: The valid SRT content
                            """
                            
                            if replaced_srt and replaced_srt.strip():
                                return replaced_srt.strip()
                            else:
                                return video_srt.strip()

                        # ç‚¹å‡»äº‹ä»¶ä¸­è°ƒç”¨ summarize_asrï¼ŒåŠ¨æ€é€‰æ‹©æœ‰æ•ˆçš„ SRT å†…å®¹
                        summarize_button.click(
                            lambda system, user, video_srt, replaced_srt, model, apikey, api_base: summarize_asr(
                                system,
                                user,
                                get_valid_srt_output(video_srt, replaced_srt),
                                model,
                                apikey,
                                api_base
                            ),
                            inputs=[
                                system_prompt_input,      # ç³»ç»Ÿæç¤ºè¯
                                user_prompt_input,        # ç”¨æˆ·æç¤ºè¯
                                video_srt_output,         # åŸå§‹ SRT å†…å®¹
                                replaced_srt_output,      # æ›¿æ¢åçš„ SRT å†…å®¹
                                llm_model,                # é€‰æ‹©çš„ LLM æ¨¡å‹
                                apikey_input,             # API Key
                                api_base_input            # API Base URL
                            ],
                            outputs=[llm_summary_result]  # è¾“å‡ºç»“æœ
                        )

        recog_button.click(mix_recog, 
                            inputs=[video_input, 
                                    audio_input, 
                                    hotwords_input, 
                                    output_dir,
                                    ], 
                            outputs=[video_text_output, video_srt_output, video_state, audio_state])
        recog_button2.click(mix_recog_speaker, 
                            inputs=[video_input, 
                                    audio_input, 
                                    hotwords_input, 
                                    output_dir,
                                    ], 
                            outputs=[video_text_output, video_srt_output, video_state, audio_state])

    # start gradio service in local or share
    if args.listen:
        funclip_service.launch(
            share=args.share, 
            server_port=args.port, 
            server_name=server_name, 
            inbrowser=False,
            auth=(DEFAULT_USERNAME, DEFAULT_PASSWORD)  # æ·»åŠ è´¦å·å’Œå¯†ç è®¤è¯
        )
    else:
        funclip_service.launch(
            share=args.share, 
            server_port=args.port, 
            server_name=server_name,
            auth=(DEFAULT_USERNAME, DEFAULT_PASSWORD)  # æ·»åŠ è´¦å·å’Œå¯†ç è®¤è¯
        )