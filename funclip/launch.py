#!/usr/bin/env python3
# -*- encoding: utf-8 -*-
# Copyright FunASR (https://github.com/alibaba-damo-academy/FunClip). All Rights Reserved.
#  MIT License  (https://opensource.org/licenses/MIT)

from http import server
import os
import logging
import argparse
import gradio as gr
from funasr import AutoModel
from videoclipper import VideoClipper
from llm.openai_api import openai_call
from llm.qwen_api import call_qwen_model
from llm.g4f_openai_api import g4f_openai_call
from utils.trans_utils import extract_timestamps
from introduction import top_md_1, top_md_3, top_md_4


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='argparse testing')
    parser.add_argument('--lang', '-l', type=str, default = "zh", help="language")
    parser.add_argument('--share', '-s', action='store_true', help="if to establish gradio share link")
    parser.add_argument('--port', '-p', type=int, default=7860, help='port number')
    parser.add_argument('--listen', action='store_true', help="if to listen to all hosts")
    args = parser.parse_args()
    
    if args.lang == 'zh':
        funasr_model = AutoModel(model="iic/speech_seaco_paraformer_large_asr_nat-zh-cn-16k-common-vocab8404-pytorch",
                                vad_model="damo/speech_fsmn_vad_zh-cn-16k-common-pytorch",
                                punc_model="damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch",
                                spk_model="damo/speech_campplus_sv_zh-cn_16k-common",
                                )
    else:
        funasr_model = AutoModel(model="iic/speech_paraformer_asr-en-16k-vocab4199-pytorch",
                                vad_model="damo/speech_fsmn_vad_zh-cn-16k-common-pytorch",
                                punc_model="damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch",
                                spk_model="damo/speech_campplus_sv_zh-cn_16k-common",
                                )
    audio_clipper = VideoClipper(funasr_model)
    audio_clipper.lang = args.lang
    
    server_name='127.0.0.1'
    if args.listen:
        server_name = '0.0.0.0'
        
        

    def audio_recog(audio_input, sd_switch, hotwords, output_dir):
        return audio_clipper.recog(audio_input, sd_switch, None, hotwords, output_dir=output_dir)

    def video_recog(video_input, sd_switch, hotwords, output_dir):
        return audio_clipper.video_recog(video_input, sd_switch, hotwords, output_dir=output_dir)

    def video_clip(dest_text, video_spk_input, start_ost, end_ost, state, output_dir):
        return audio_clipper.video_clip(
            dest_text, start_ost, end_ost, state, dest_spk=video_spk_input, output_dir=output_dir
            )

    def mix_recog(video_input, audio_input, hotwords, output_dir):
        output_dir = output_dir.strip()
        if not len(output_dir):
            output_dir = None
        else:
            output_dir = os.path.abspath(output_dir)
        audio_state, video_state = None, None
        if video_input is not None:
            res_text, res_srt, video_state = video_recog(
                video_input, 'No', hotwords, output_dir=output_dir)
            return res_text, res_srt, video_state, None
        if audio_input is not None:
            res_text, res_srt, audio_state = audio_recog(
                audio_input, 'No', hotwords, output_dir=output_dir)
            return res_text, res_srt, None, audio_state
    
    def mix_recog_speaker(video_input, audio_input, hotwords, output_dir):
        output_dir = output_dir.strip()
        if not len(output_dir):
            output_dir = None
        else:
            output_dir = os.path.abspath(output_dir)
        audio_state, video_state = None, None
        if video_input is not None:
            res_text, res_srt, video_state = video_recog(
                video_input, 'Yes', hotwords, output_dir=output_dir)
            return res_text, res_srt, video_state, None
        if audio_input is not None:
            res_text, res_srt, audio_state = audio_recog(
                audio_input, 'Yes', hotwords, output_dir=output_dir)
            return res_text, res_srt, None, audio_state
    
    def mix_clip(dest_text, video_spk_input, start_ost, end_ost, video_state, audio_state, output_dir):
        output_dir = output_dir.strip()
        if not len(output_dir):
            output_dir = None
        else:
            output_dir = os.path.abspath(output_dir)
        if video_state is not None:
            clip_video_file, message, clip_srt = audio_clipper.video_clip(
                dest_text, start_ost, end_ost, video_state, dest_spk=video_spk_input, output_dir=output_dir)
            return clip_video_file, None, message, clip_srt
        if audio_state is not None:
            (sr, res_audio), message, clip_srt = audio_clipper.clip(
                dest_text, start_ost, end_ost, audio_state, dest_spk=video_spk_input, output_dir=output_dir)
            return None, (sr, res_audio), message, clip_srt
    
    def video_clip_addsub(dest_text, video_spk_input, start_ost, end_ost, state, output_dir, font_size, font_color):
        output_dir = output_dir.strip()
        if not len(output_dir):
            output_dir = None
        else:
            output_dir = os.path.abspath(output_dir)
        return audio_clipper.video_clip(
            dest_text, start_ost, end_ost, state, 
            font_size=font_size, font_color=font_color, 
            add_sub=True, dest_spk=video_spk_input, output_dir=output_dir
            )
        
    def llm_inference(system_content, user_content, srt_text, model, apikey):
        SUPPORT_LLM_PREFIX = ['qwen', 'gpt', 'g4f', 'moonshot']
        if model.startswith('qwen'):
            return call_qwen_model(apikey, model, user_content+'\n'+srt_text, system_content)
        if model.startswith('gpt') or model.startswith('moonshot'):
            return openai_call(apikey, model, system_content, user_content+'\n'+srt_text)
        elif model.startswith('g4f'):
            model = "-".join(model.split('-')[1:])
            return g4f_openai_call(model, system_content, user_content+'\n'+srt_text)
        else:
            logging.error("LLM name error, only {} are supported as LLM name prefix."
                          .format(SUPPORT_LLM_PREFIX))
    
    def AI_clip(LLM_res, dest_text, video_spk_input, start_ost, end_ost, video_state, audio_state, output_dir):
        timestamp_list = extract_timestamps(LLM_res)
        output_dir = output_dir.strip()
        if not len(output_dir):
            output_dir = None
        else:
            output_dir = os.path.abspath(output_dir)
        if video_state is not None:
            clip_video_file, message, clip_srt = audio_clipper.video_clip(
                dest_text, start_ost, end_ost, video_state, 
                dest_spk=video_spk_input, output_dir=output_dir, timestamp_list=timestamp_list, add_sub=False)
            return clip_video_file, None, message, clip_srt
        if audio_state is not None:
            (sr, res_audio), message, clip_srt = audio_clipper.clip(
                dest_text, start_ost, end_ost, audio_state, 
                dest_spk=video_spk_input, output_dir=output_dir, timestamp_list=timestamp_list, add_sub=False)
            return None, (sr, res_audio), message, clip_srt
    
    def AI_clip_subti(LLM_res, dest_text, video_spk_input, start_ost, end_ost, video_state, audio_state, output_dir):
        timestamp_list = extract_timestamps(LLM_res)
        output_dir = output_dir.strip()
        if not len(output_dir):
            output_dir = None
        else:
            output_dir = os.path.abspath(output_dir)
        if video_state is not None:
            clip_video_file, message, clip_srt = audio_clipper.video_clip(
                dest_text, start_ost, end_ost, video_state, 
                dest_spk=video_spk_input, output_dir=output_dir, timestamp_list=timestamp_list, add_sub=True)
            return clip_video_file, None, message, clip_srt
        if audio_state is not None:
            (sr, res_audio), message, clip_srt = audio_clipper.clip(
                dest_text, start_ost, end_ost, audio_state, 
                dest_spk=video_spk_input, output_dir=output_dir, timestamp_list=timestamp_list, add_sub=True)
            return None, (sr, res_audio), message, clip_srt
        
    def extract_speaker_labels(subtitles):
        """
        ä»å­—å¹•å†…å®¹ä¸­æå–æ‰€æœ‰çš„ spkX æ ‡è¯†ï¼Œå¹¶ç”Ÿæˆé»˜è®¤çš„æ˜ å°„å­—ç¬¦ä¸²ã€‚
        :param subtitles: SRTå­—å¹•å†…å®¹
        :return: é»˜è®¤çš„è¯´è¯äººæ˜ å°„å­—ç¬¦ä¸² (å¦‚ "spk0:, spk1:, spk2:")
        """
        import re
        speakers = sorted(set(re.findall(r"\bspk\d+\b", subtitles)))
        return "\n".join(f"{speaker}:" for speaker in speakers)

    def prepare_speaker_map(subtitles):
        """
        æ ¹æ®å­—å¹•å†…å®¹åŠ¨æ€ç”Ÿæˆé¢„å¡«å……çš„è¯´è¯äººæ˜ å°„è§„åˆ™ã€‚
        :param subtitles: å½“å‰ SRT å­—å¹•å†…å®¹
        :return: é»˜è®¤çš„è¯´è¯äººæ˜ å°„è§„åˆ™å­—ç¬¦ä¸²
        """
        return extract_speaker_labels(subtitles)

    def parse_speaker_map(speaker_map_str):
        """
        è§£æç”¨æˆ·è¾“å…¥çš„è¯´è¯äººæ˜ å°„å­—ç¬¦ä¸²ä¸ºå­—å…¸ã€‚
        :param speaker_map_str: è¯´è¯äººæ˜ å°„å­—ç¬¦ä¸²ï¼Œæ¯è¡Œä¸€ä¸ªæ˜ å°„ (å¦‚ "spk0:å¼ ä¸‰\nspk1:\nspk2:ç‹äº”")
        :return: è§£æåçš„å­—å…¸ (å¦‚ {"spk0": "å¼ ä¸‰", "spk2": "ç‹äº”"})
        """
        try:
            return {
                item.split(":", 1)[0].strip(): item.split(":", 1)[1].strip()
                for item in speaker_map_str.splitlines()
                if ":" in item.strip() and item.split(":", 1)[1].strip()  # å¿½ç•¥ç©ºå€¼è¡Œ
            }
        except ValueError:
            raise ValueError("è¯´è¯äººæ˜ å°„æ ¼å¼é”™è¯¯ï¼Œè¯·è¾“å…¥æ­£ç¡®çš„æ ¼å¼ï¼Œä¾‹å¦‚æ¯è¡Œ 'spk0:å¼ ä¸‰'")


    def replace_speaker_in_subtitles(subtitles, speaker_map_str):
        """
        æ›¿æ¢å­—å¹•ä¸­çš„è¯´è¯äººæ ‡è¯†ã€‚
        :param subtitles: åŸå§‹SRTå­—å¹•å†…å®¹ï¼ˆå­—ç¬¦ä¸²ï¼‰
        :param speaker_map_str: ç”¨æˆ·æä¾›çš„è¯´è¯äººæ˜ å°„å­—ç¬¦ä¸² (å¦‚ "spk0:å¼ ä¸‰, spk1:æå››")
        :return: æ›¿æ¢åçš„SRTå­—å¹•å†…å®¹
        """
        # å°†æ˜ å°„å­—ç¬¦ä¸²è§£æä¸ºå­—å…¸
        speaker_map = parse_speaker_map(speaker_map_str)
        
        # æ›¿æ¢SRTå†…å®¹ä¸­çš„è¯´è¯äºº
        def replace_speaker_line(line):
            for old_speaker, new_speaker in speaker_map.items():
                line = line.replace(old_speaker, new_speaker)
            return line

        # é€è¡Œæ›¿æ¢SRTå†…å®¹
        lines = subtitles.split("\n")
        replaced_lines = [replace_speaker_line(line) for line in lines]
        return "\n".join(replaced_lines)

    def summarize_asr(system_prompt, user_prompt, asr_text, model, apikey):
        """
        è°ƒç”¨LLMæ€»ç»“ASRè¯†åˆ«å†…å®¹
        :param system_prompt: ç³»ç»Ÿè®¾å®šçš„Prompt
        :param user_prompt: ç”¨æˆ·è¾“å…¥çš„Prompt
        :param asr_text: ASRè¯†åˆ«çš„ç»“æœ
        :param model: LLMæ¨¡å‹åç§°
        :param apikey: ç”¨æˆ·æä¾›çš„APIå¯†é’¥
        :return: LLMè¿”å›çš„æ€»ç»“ç»“æœ
        """
        return llm_inference(system_prompt, user_prompt, asr_text, model, apikey)


    # gradio interface
    theme = gr.Theme.load("funclip/utils/theme.json")
    with gr.Blocks(theme=theme) as funclip_service:

        video_state, audio_state = gr.State(), gr.State()
        with gr.Row():
            with gr.Column():
                with gr.Row():
                    video_input = gr.Video(label="è§†é¢‘è¾“å…¥ | Video Input")
                    audio_input = gr.Audio(label="éŸ³é¢‘è¾“å…¥ | Audio Input")
                with gr.Column():
                    gr.Examples(['https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ClipVideo/%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%A4%9A%E8%AF%BB%E4%B9%A6%EF%BC%9F%E8%BF%99%E6%98%AF%E6%88%91%E5%90%AC%E8%BF%87%E6%9C%80%E5%A5%BD%E7%9A%84%E7%AD%94%E6%A1%88-%E7%89%87%E6%AE%B5.mp4', 
                                 'https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ClipVideo/2022%E4%BA%91%E6%A0%96%E5%A4%A7%E4%BC%9A_%E7%89%87%E6%AE%B52.mp4', 
                                 'https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ClipVideo/%E4%BD%BF%E7%94%A8chatgpt_%E7%89%87%E6%AE%B5.mp4'],
                                [video_input],
                                label='ç¤ºä¾‹è§†é¢‘ | Demo Video')
                    gr.Examples(['https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ClipVideo/%E8%AE%BF%E8%B0%88.mp4'],
                                [video_input],
                                label='å¤šè¯´è¯äººç¤ºä¾‹è§†é¢‘ | Multi-speaker Demo Video')
                    gr.Examples(['https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ClipVideo/%E9%B2%81%E8%82%83%E9%87%87%E8%AE%BF%E7%89%87%E6%AE%B51.wav'],
                                [audio_input],
                                label="ç¤ºä¾‹éŸ³é¢‘ | Demo Audio")
                    with gr.Column():
                        # with gr.Row():
                            # video_sd_switch = gr.Radio(["No", "Yes"], label="ğŸ‘¥åŒºåˆ†è¯´è¯äºº Get Speakers", value='No')
                        hotwords_input = gr.Textbox(label="ğŸš’ çƒ­è¯ | Hotwords(å¯ä»¥ä¸ºç©ºï¼Œå¤šä¸ªçƒ­è¯ä½¿ç”¨ç©ºæ ¼åˆ†éš”ï¼Œä»…æ”¯æŒä¸­æ–‡çƒ­è¯)")
                        output_dir = gr.Textbox(label="ğŸ“ æ–‡ä»¶è¾“å‡ºè·¯å¾„ | File Output Dir (å¯ä»¥ä¸ºç©ºï¼ŒLinux, macç³»ç»Ÿå¯ä»¥ç¨³å®šä½¿ç”¨)", value=" ")
                        with gr.Row():
                            recog_button = gr.Button("ğŸ‘‚ è¯†åˆ« | ASR", variant="primary")
                            recog_button2 = gr.Button("ğŸ‘‚ğŸ‘« è¯†åˆ«+åŒºåˆ†è¯´è¯äºº | ASR+SD")
                video_text_output = gr.Textbox(label="âœï¸ è¯†åˆ«ç»“æœ | Recognition Result")
                video_srt_output = gr.Textbox(label="ğŸ“– SRTå­—å¹•å†…å®¹ | SRT Subtitles")
            with gr.Column():
                # æ›¿æ¢è¯´è¯äºº Tab
                with gr.Tab("ğŸ”„ æ›¿æ¢è¯´è¯äºº Replace Speaker"):
                    speaker_map_input = gr.Textbox(
                        label="æ›¿æ¢è§„åˆ™ | Replacement Rules (æ¯è¡Œæ ¼å¼: spkX:åç§°)",
                        placeholder="è‡ªåŠ¨ç”Ÿæˆå½“å‰å­—å¹•çš„spkXæ ‡è¯†...",
                        lines=10,  # å…è®¸å¤šè¡Œè¾“å…¥
                    )
                    replace_button = gr.Button("æ›¿æ¢ Replace", variant="primary")
                    replaced_srt_output = gr.Textbox(label="æ›¿æ¢åçš„SRTå­—å¹•å†…å®¹ | Replaced SRT Subtitles")

                    # è‡ªåŠ¨å¡«å…… speaker_map_input çš„é»˜è®¤å€¼
                    video_srt_output.change(
                        prepare_speaker_map,
                        inputs=[video_srt_output],
                        outputs=[speaker_map_input],
                    )

                    replace_button.click(
                        replace_speaker_in_subtitles,
                        inputs=[video_srt_output, speaker_map_input],
                        outputs=[replaced_srt_output],
                    )

                with gr.Tab("ğŸ“„ LLMæ–‡æ¡£æ€»ç»“ | LLM Document Summarization"):
                    with gr.Column():
                        system_prompt_input = gr.Textbox(
                            label="Prompt System (ç³»ç»Ÿæç¤ºè¯)",
                            placeholder="è¯·è¾“å…¥ç³»ç»ŸPromptï¼Œä¾‹å¦‚ï¼šä½ æ˜¯ä¸€ä¸ªè¯­éŸ³è¯†åˆ«æ€»ç»“åŠ©æ‰‹...",
                            value=("ä½ æ˜¯ä¸€ä¸ªè¯­éŸ³è¯†åˆ«æ€»ç»“åŠ©æ‰‹ï¼Œæ¥æ”¶ç”¨æˆ·æä¾›çš„è¯­éŸ³è½¬æ–‡æœ¬ï¼ˆASRï¼‰ç»“æœï¼Œ"
                                "æ ¹æ®ç”¨æˆ·æŒ‡ä»¤æ€»ç»“å…³é”®ä¿¡æ¯æˆ–ç”Ÿæˆå†…å®¹ã€‚")
                        )
                        user_prompt_input = gr.Textbox(
                            label="Prompt User (ç”¨æˆ·è‡ªå®šä¹‰æç¤ºè¯)",
                            placeholder="è¯·è¾“å…¥ç”¨æˆ·Promptï¼Œä¾‹å¦‚ï¼šæ€»ç»“ä»¥ä¸‹å†…å®¹çš„å…³é”®ä¿¡æ¯...",
                            value="æ€»ç»“ä»¥ä¸‹å†…å®¹çš„å…³é”®ä¿¡æ¯ï¼š"
                        )
                        llm_model = gr.Dropdown(
                            choices=["qwen-plus",
                                    "gpt-3.5-turbo",
                                    "gpt-3.5-turbo-0125",
                                    "gpt-4-turbo",
                                    "g4f-gpt-3.5-turbo"],
                            value="qwen-plus",
                            label="LLM Model Name",
                            allow_custom_value=True
                        )
                        apikey_input = gr.Textbox(
                            label="APIKEY",
                            placeholder="è¾“å…¥API Keyï¼ˆå¦‚éœ€ä½¿ç”¨GPTæˆ–Qwen APIï¼‰"
                        )
                        summarize_button = gr.Button(
                            "æ€»ç»“ Summarize (ä½¿ç”¨LLMæ€»ç»“ASRå†…å®¹)",
                            variant="primary"
                        )
                        llm_summary_result = gr.Textbox(
                            label="æ€»ç»“ç»“æœ | Summary Result",
                            placeholder="LLMè¿”å›çš„æ€»ç»“ç»“æœå°†æ˜¾ç¤ºåœ¨è¿™é‡Œ"
                        )

                        # å°†è¾“å…¥çš„ASRæ–‡æœ¬ç”¨äºæ€»ç»“
                        summarize_button.click(
                            summarize_asr,
                            inputs=[system_prompt_input, user_prompt_input, video_text_output, llm_model, apikey_input],
                            outputs=[llm_summary_result]
                        )

        recog_button.click(mix_recog, 
                            inputs=[video_input, 
                                    audio_input, 
                                    hotwords_input, 
                                    output_dir,
                                    ], 
                            outputs=[video_text_output, video_srt_output, video_state, audio_state])
        recog_button2.click(mix_recog_speaker, 
                            inputs=[video_input, 
                                    audio_input, 
                                    hotwords_input, 
                                    output_dir,
                                    ], 
                            outputs=[video_text_output, video_srt_output, video_state, audio_state])

    # start gradio service in local or share
    if args.listen:
        funclip_service.launch(share=args.share, server_port=args.port, server_name=server_name, inbrowser=False)
    else:
        funclip_service.launch(share=args.share, server_port=args.port, server_name=server_name)
